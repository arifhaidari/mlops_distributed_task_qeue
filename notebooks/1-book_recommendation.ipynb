{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommendation algorithms commonly have the following architecture:\n",
    "\n",
    "Generation of candidates. This first step is responsible for generating the subsets of candidates to recommend to the user. For example, among all the books we take a subset of more relevant books from which we will recommend.\n",
    "\n",
    "Scoring. Another model is responsible for scoring and ranking the candidates in order to choose the set of products to present to the user.\n",
    "\n",
    "Reclassification. Additionally, the system takes into account additional constraints for the final classification. For example, if the user did not like a certain product we delete it or if there is a more recent product we increase their score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The different approaches\n",
    "We can distinguish several approaches, in this module we will mainly focus on:\n",
    "1- content based methods, 2 - collaborative filtering methods (memory approach and model approach), 3 - Hybrid methods <br>\n",
    "collaborative filtering methods: <br>\n",
    "In content-based filtering we use known information about users' interests as a link for potential recommendations.\n",
    "\n",
    "Suppose Alice is asked what type of books she likes to see if she would be interested in The Hobbit. With this information, we start by labeling the users as well as the items, in this case books with known variables, for example adventure and romance.\n",
    "\n",
    "If Alice doesn't like adventure but she likes romance, we can represent her preferences as the vector  (0,4),\n",
    "  assuming a notation between  0−5.\n",
    " \n",
    "We can do the same for books: The Hobbit does not contain romance but contains a lot of adventures, this gives let's say the vector  (5,0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset link\n",
    "# https://www.kaggle.com/datasets/thedevastator/comprehensive-overview-of-52478-goodreads-best-b\n",
    "\n",
    "\n",
    "# other dataset for exploration:\n",
    "# https://www.kaggle.com/datasets/jealousleopard/goodreadsbooks\n",
    "# https://cseweb.ucsd.edu/~jmcauley/datasets/goodreads.html#datasets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we decide to keep only the 'book_id', 'title', 'language_code' and 'description' columns. We keep the language code so that we can filter the books and keep those that are in English only (predominant category in the database). As for the description, we keep it so that we can extract the characteristics of each book and thus make a recommendation based on their content.\n",
    "\n",
    "We also notice that there are books which have missing values ​​in the language_code column. To ensure more precise filtering, we also use the description column to filter books in English. There are functions on Python like detect from the langdetect library which allow you, given a text, to identify the language in which it is written.\n",
    "\n",
    "Once the pre-processing is done, we only keep the 'book_id', 'title' and 'description' columns for our recommendation system. The file 'goodreads_descr' contains the cleaned database with information on just over 9000 books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "df = pd.read_csv('goodreads_descr.csv')\n",
    "\n",
    "# Displaying the first 10 lines\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we read the summary of a book, we find important elements which allow us to define whether it could interest us or not. If we like the fantasy genre, we might be more attracted to descriptions containing words such as 'magic', 'dragons' or 'elf'.\n",
    "\n",
    "For our recommendation system we will use the ''description'' column to find these words and thus identify books that cover similar subjects.\n",
    "\n",
    "To do this, we will go through tokenization and vectorization (a subject which will be covered in depth on the Text Mining module). For now, all you need to know is that these are fundamental processes in natural language processing (NLP) which consist of converting textual data into a format understandable and usable by Machine Learning models.\n",
    "\n",
    "Tokenization is the process of dividing a text into individual units (words or subwords) called \"tokens\".\n",
    "\n",
    "Vectorization consists of representing each \"token\" in digital form, generally in the form of a vector of numbers. This is necessary because most machine learning models require numerical input.\n",
    "\n",
    "TfidfVectorizer is a scikit-learn package that performs both tokenization and TF-IDF (Term-Frequency-Inverse Document) vectorization to create digital representations of documents.\n",
    "\n",
    "The stop_words parameter removes the empty words that are very frequent and therefore not significant in a text. Removing them does not represent a significant loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TfidfVectorizer and remove stop words\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Adapt and transform the data into a tfidf matrix\n",
    "tfidf_matrix = tfidf.fit_transform(df['description'])\n",
    "\n",
    "# Show tfidf matrix shape\n",
    "print(tfidf_matrix.shape)\n",
    "display(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of lines (9676) corresponds to the number of books in our DataFrame. As for the number of columns (53433), this corresponds to the number of important words after removing stop words.\n",
    "\n",
    "Then we will calculate the similarity between each book.\n",
    "\n",
    "The cosine_similarity and euclidean_distances functions from the sklearn.metrics.pairwise library calculate the cosine similarity and Euclidean distance, respectively.\n",
    "Attention: Remember that in the Euclidean measurement, the shorter the distance, the higher the similarity. To ensure that higher values ​​correspond to greater similarity (common convention in recommendation systems) we do the transformation euclidean_sim = 1 / (1 + euclidean_distances(tfidf_matrix)). Also note that the Euclidean measure is never recommended for very large dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "# We calculate the cosine similarity\n",
    "sim_cosine = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# We calculate the Euclidean similarity\n",
    "sim_euclidean = 1 / (1 + euclidean_distances(tfidf_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pandas series named indexes using the 'title' column of the DataFrame as index. This series will match the index to the title of the associated book when recommending."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a series of indices using the 'title' column as an index\n",
    "indices = pd.Series(range(0, len(df)), index=df['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a function recommendations which will take as input:\n",
    "title: from which we will recommend it.\n",
    "mat_sim: the similarity matrix calculated previously.\n",
    "num_recommendations: the number of recommendations to return, we can set the default to 10.\n",
    "The function must retrieve in a variable named idx the index associated with the title from the series indices.\n",
    "\n",
    "Then it will have to keep in a list the similarity scores corresponding to the index of the target film and associate each score with its index using the enumerate function and the similarity matrix mat_sim.\n",
    "\n",
    "It will have to sort the similarity scores, find the most similar ones and recover her clues.\n",
    "\n",
    "Finally, based on the clues found, return the titles of the most similar books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "def recommendations(title, mat_sim, num_recommendations = 10):\n",
    "    # We retrieve the index associated with the title which will be used to identify the book in the similarity matrix\n",
    "    idx = indices[title]\n",
    "\n",
    "    # We obtain the similarity scores of all the books with the given book and we keep the tuples of book index and score in a list\n",
    "    similarity_scores = list(enumerate(mat_sim[idx]))\n",
    "\n",
    "    # We sort books based on similarity scores\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get scores of the 10 most similar books\n",
    "    top_similair = similarity_scores[1:num_recommendations+1]\n",
    "\n",
    "    # Get book clues\n",
    "    res = [(indices.index[idx], score) for idx, score in top_similair]\n",
    "\n",
    "    # Return the titles of the most similar books\n",
    "    return tabulate(res, headers=[\"Titre\", \"Score de similarité\"], tablefmt=\"pretty\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using the recommendations function, find recommendations for the books 'Batman Detective Comics #39' and 'The Queen's Gambit' using Euclidean and cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recommendations for 'Batman Detective Comics #39' Cosine similarity: \\n\", recommendations('Batman Detective Comics #39', sim_cosine))\n",
    "print(\"\\nRecommendations for 'The Queen\\'s Gambit' Euclidean similarity: \\n\", recommendations('Batman Detective Comics #39', sim_euclidean))\n",
    "\n",
    "print(\"\\nRecommendations for 'The Queen\\'s Gambit' cosine similarity: \\n\", recommendations('The Queen\\'s Gambit', sim_cosine))\n",
    "print(\"\\nRecommendations for 'The Queen\\'s Gambit' Euclidean similarity: \\n\", recommendations('The Queen\\'s Gambit', sim_euclidean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that we have good recommendations but we also have some recommendations that don't have much to do with the target books: 'Pugs in Public' doesn't have much to do with 'Batman Detective Comics #39' and 'The Mammoth Book of Tasteless Jokes' has little to do with 'The Queen's Gambit'.\n",
    "\n",
    "   Limitation: Content-based filtering has limitations related to excessive personalization, limited discovery, and reliance on product features. It is limited to the characteristics and information present in the products themselves. If a product does not have relevant information for analysis (for example, an empty description), it may not be recommended correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Collaborative filtering¶\n",
    "Sometimes we can't explain why we like certain things, we just like them. Taking this into consideration, the main idea of ​​collaborative filtering is that a person is likely to like what people with similar interests have liked.\n",
    "\n",
    "We have two different approaches when we talk about collaborative filtering:\n",
    "\n",
    "Memory approach. We focus on the rating matrix, where users interact with the products. We will explore two methods:\n",
    "\n",
    "a. User-user. : If the books that Alice likes are similar to those of Bob, we will recommend to Alice the books that Bob liked and vice versa.\n",
    "\n",
    "b. Item-item. : If Alice likes Pride and Prejudice and it is similar to Persuasion and Emma, ​​then the latter two will be recommended to her.\n",
    "\n",
    "Model approach We use Machine Learning models to try to predict the ratings that a user will give to a product. The main method is Matrix Factorization.\n",
    "\n",
    "Here, we use the rating matrix that users were able to give to the products, even if there are missing values. From it we try to reconstruct the matrices of the variables, so that from them we find not only the values ​​that we already knew, but an estimate of the missing values ​​as well.\n",
    "\n",
    "It is important to note that, unlike the variable matrices of content-based filtering, we will not know exactly to which characteristics the entries of the variable matrices found correspond (for example we will not know exactly if there are acts of adventure or romance). We will simply call them latent variables because they come from the pattern of the data. This notion is the main difference between content-based filtering and collaborative filtering.\n",
    "\n",
    "   Limitation: cold start problem where it is difficult to make recommendations to new users or on new products.\n",
    "What to remember \n",
    "There are different approaches when we talk about recommendation systems: in this notebook we focused on content-based filtering. In future notebooks we will explore collaborative filtering in more detail.\n",
    "\n",
    "A similarity measure is a function  𝑠\n",
    "  that takes a pair of embeddings  𝑥,𝑦\n",
    "  and returns a scalar value measuring their similarity.\n",
    "\n",
    "Content-based filtering makes it possible to make recommendations that are based on the intrinsic information of products or users, therefore limiting itself to having this information beforehand."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
